{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e14f36-0f86-4fc1-ac74-41805c22f3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                              \u001b[0m\u001b[01;34mLIG-Doctor\u001b[0m/                    strataboy.ipynb\n",
      "\u001b[01;34mapplicationcluster\u001b[0m/            LoadAndTrain.ipynb             test.ipynb\n",
      "\u001b[01;34mbatchFiles\u001b[0m/                    \u001b[01;32mmajup\u001b[0m*                         train_eval.ipynb\n",
      "\u001b[01;34mbin\u001b[0m/                           \u001b[01;34mmimic-iv-2.2\u001b[0m/                  Training.ipynb\n",
      "\u001b[01;34mClinical-GAN\u001b[0m/                  \u001b[01;34mOutFiles\u001b[0m/                      Training.py\n",
      "\u001b[01;34mCSS\u001b[0m/                           \u001b[01;34mPatientTrajectoryForecasting\u001b[0m/  Untitled.ipynb\n",
      "data-gen-mimic-iv_works.ipynb  PrepareData.ipynb              \u001b[01;34mwandb\u001b[0m/\n",
      "\u001b[01;34mDataImportants\u001b[0m/                RisingThunder.ipynb            zifSnXUB\n",
      "\u001b[01;34mdoctorai\u001b[0m/                      seeker.ipynb\n",
      "\u001b[01;34mfinal\u001b[0m/                         slurm-5653.out\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa273a12-d8e7-45cc-b60c-40b6518a7042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sifal.klioui/PatientTrajectoryForecasting\n"
     ]
    }
   ],
   "source": [
    "cd PatientTrajectoryForecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "113e0fe3-c61f-41cc-b671-e18b1a0f6041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62e714e2-085d-4269-86a8-f2d59113b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from utils.train import train_epoch, evaluate, get_data_loaders\n",
    "import wandb\n",
    "import argparse\n",
    "import os\n",
    "from model import Seq2SeqTransformer\n",
    "from utils.eval import get_sequences, mapk\n",
    "import warnings\n",
    "# currently getting warnings because of mask datatypes, you might wanna change this not installing from environment.yml\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1820594a-4d82-49e8-9dbf-3ec6b0edbeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataConfig:\n",
    "    strategy : str = 'SDP'\n",
    "    seed : int = 89957\n",
    "    test_size : float = 0.05\n",
    "    valid_size : float = 0.10\n",
    "    predict_procedure : bool = None\n",
    "    predict_drugs : bool = None\n",
    "    input_max_length :int = 448\n",
    "    target_max_length :int = 64\n",
    "    source_vocab_size : int = None\n",
    "    target_vocab_size : int = None\n",
    "    target_pad_id : int = 0\n",
    "    source_pad_id : int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f881020-bd44-4845-9a81-c54138093d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    positional_encoding : bool = True\n",
    "    num_encoder_layers: int = 6\n",
    "    num_decoder_layers: int = 6\n",
    "    nhead: int = 8\n",
    "    emb_size: int = 1024\n",
    "    ffn_hid_dim: int = 1024\n",
    "    dropout: float = 0.1\n",
    "    train_batch_size: int = 64\n",
    "    eval_batch_size: int = 128\n",
    "    learning_rate: float = 0.0001\n",
    "    warmup_start: float = 5\n",
    "    num_train_epochs: int = 45\n",
    "    warmup_epochs: int = None\n",
    "    label_smoothing : float = 0.05\n",
    "    scheduler : str = 'StepLR'\n",
    "    factor : float = 0.1\n",
    "    patience : int = 5\n",
    "    T_0 : int = 10\n",
    "    T_mult : int = 2\n",
    "    step_size : int = 10\n",
    "    gamma : float = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a2f9c74-1358-48b3-ac6a-cfa168feff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sifal.klioui\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f090b1d3-303d-4d6f-8ce3-b8315b3de6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_to_old_ids_source file not availble, mapping is the same as the old on\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "data_config = DataConfig()\n",
    "train_dataloader, val_dataloader, test_dataloader, src_tokens_to_ids, tgt_tokens_to_ids, _, data_and_properties  = get_data_loaders(**asdict(data_config))\n",
    "\n",
    "data_config.source_vocab_size = data_and_properties['embedding_size_source']\n",
    "data_config.target_vocab_size = data_and_properties['embedding_size_target']\n",
    "data_config.target_pad_id = tgt_tokens_to_ids['PAD']\n",
    "data_config.source_pad_id = src_tokens_to_ids['PAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1aa22ae-7388-4817-bde1-aad68fe1981b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sifal.klioui/PatientTrajectoryForecasting\n"
     ]
    }
   ],
   "source": [
    "cd PatientTrajectoryForecasting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "27c26998-088f-4df7-93bd-0faf1ffaf839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sifal.klioui/PatientTrajectoryForecasting/wandb/run-20240503_114217-w2sovwq9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sifalklioui/PTF_SDP_D/runs/w2sovwq9' target=\"_blank\">drawn-meadow-65</a></strong> to <a href='https://wandb.ai/sifalklioui/PTF_SDP_D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sifalklioui/PTF_SDP_D' target=\"_blank\">https://wandb.ai/sifalklioui/PTF_SDP_D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sifalklioui/PTF_SDP_D/runs/w2sovwq9' target=\"_blank\">https://wandb.ai/sifalklioui/PTF_SDP_D/runs/w2sovwq9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sifalklioui/PTF_SDP_D/runs/w2sovwq9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4d5ce98b10>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"PTF_SDP_D\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config=asdict(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fc947cbf-8aec-4730-b0a8-b31ae0c07676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params:  106.92m\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer = Seq2SeqTransformer(config.num_encoder_layers, config.num_decoder_layers, config.emb_size,\n",
    "                             config.nhead, data_config.source_vocab_size,\n",
    "                             data_config.target_vocab_size, config.ffn_hid_dim,\n",
    "                            config.dropout,\n",
    "                            config.positional_encoding)\n",
    "\n",
    "print(f'number of params: {sum(p.numel() for p in transformer.parameters()) / 1e6 : 0.2f}m')\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    transformer = nn.DataParallel(transformer)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index= data_config.target_pad_id, label_smoothing = config.label_smoothing)\n",
    "\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Select the scheduler based on configuration\n",
    "if config.scheduler == 'StepLR':\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=config.step_size, gamma=config.gamma)\n",
    "elif config.scheduler == 'ReduceLROnPlateau':\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=config.factor, patience=config.patience)\n",
    "elif config.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "    scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=config.T_0, T_mult=config.T_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "83696204-e12b-49e5-809d-e2ebe224f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [20,40,72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a6b2a-1a2c-42d9-89ff-cef33bc4265e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 220/220 [06:20<00:00,  1.73s/it]\n",
      "evaluation: 100%|██████████| 26/26 [00:14<00:00,  1.82it/s]\n",
      "scoring: 100%|██████████| 26/26 [00:41<00:00,  1.58s/it]\n",
      "train: 100%|██████████| 220/220 [06:21<00:00,  1.73s/it]\n",
      "evaluation: 100%|██████████| 26/26 [00:14<00:00,  1.82it/s]\n",
      "scoring: 100%|██████████| 26/26 [00:44<00:00,  1.71s/it]\n",
      "train: 100%|██████████| 220/220 [06:21<00:00,  1.73s/it]\n",
      "evaluation: 100%|██████████| 26/26 [00:14<00:00,  1.82it/s]\n",
      "scoring: 100%|██████████| 26/26 [00:41<00:00,  1.60s/it]\n",
      "train: 100%|██████████| 220/220 [06:21<00:00,  1.73s/it]\n",
      "evaluation: 100%|██████████| 26/26 [00:14<00:00,  1.82it/s]\n",
      "scoring: 100%|██████████| 26/26 [00:43<00:00,  1.66s/it]\n",
      "train: 100%|██████████| 220/220 [06:21<00:00,  1.73s/it]\n",
      "evaluation: 100%|██████████| 26/26 [00:14<00:00,  1.82it/s]\n",
      "scoring: 100%|██████████| 26/26 [00:43<00:00,  1.67s/it]\n",
      "train:  71%|███████   | 156/220 [04:30<01:50,  1.73s/it]"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    for epoch in range(1,config.num_train_epochs):\n",
    "        val_mapk = {}\n",
    "        train_loss = train_epoch(transformer,  optimizer, train_dataloader, loss_fn, data_config.source_pad_id, data_config.target_pad_id, DEVICE)\n",
    "        val_loss =  evaluate(transformer, val_dataloader, loss_fn, data_config.source_pad_id, data_config.target_pad_id, DEVICE)\n",
    "        pred_trgs, targets =  get_sequences(transformer, val_dataloader, data_config.source_pad_id, tgt_tokens_to_ids, max_len = 72, DEVICE = DEVICE)\n",
    "        if pred_trgs:\n",
    "            val_mapk = {f\"val_map@{k}\": mapk(targets, pred_trgs, k) for k in ks}\n",
    "            wandb.log({\"Epoch\": epoch, \"train_loss\": train_loss,\"val_loss\":val_loss, \"lr\" : optimizer.param_groups[0]['lr'], **val_mapk})\n",
    "        if config.scheduler :\n",
    "        # Step the scheduler based on its type\n",
    "            if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "except Exception as e:\n",
    "        wandb.log({\"error\": str(e)})\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ad787-f6cc-4d33-97f5-6ee49ed7de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "finally:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e161022-bd4d-4805-9089-30745d02662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sifal.klioui\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475135d-852d-4e5c-9cc7-459c18fe275f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c5b304f-cbe8-4ccb-a3fe-a42b9616fe85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3303"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e957824-4b01-475a-8fae-386f2f7b68db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_map@20': 0.48360119523507766,\n",
       " 'val_map@40': 0.48428558322105636,\n",
       " 'val_map@60': 0.48382014672449825}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "763a0ea1-e213-4df2-a0c7-b2037c32c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mapk = {f\"val_map@{k}\": mapk(targets, pred_trgs, k) for k in ks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d461716-33dc-45c5-ac12-feb94da999df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.scheduler :\n",
    "# Step the scheduler based on its type\n",
    "    if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "        scheduler.step(val_loss)\n",
    "    else:\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795fb2c-c3ac-4b87-9c3e-20c5a5e55148",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329ce3e-5a1c-49cb-8845-c7113fce5b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer = Seq2SeqTransformer(config.num_encoder_layers, config.num_decoder_layers, config.emb_size,\n",
    "                             config.nhead, data_config.source_vocab_size,\n",
    "                             data_config.target_vocab_size, config.ffn_hid_dim)\n",
    "\n",
    "print(f'number of params: {sum(p.numel() for p in transformer.parameters())}')\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    transformer = nn.DataParallel(transformer)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index= data_config.target_pad_id, label_smoothing = config.label_smoothing)\n",
    "\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Select the scheduler based on configuration\n",
    "if config.scheduler == 'StepLR':\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=config.step_size, gamma=config.gamma)\n",
    "elif config.scheduler == 'ReduceLROnPlateau':\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=config.factor, patience=config.patience)\n",
    "elif config.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "    scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=config.T_0, T_mult=config.T_mult)\n",
    "\n",
    "# add wandb loss logging\n",
    "for epoch in range(config.num_train_epochs):\n",
    "    val_mapk = {}\n",
    "    train_loss = train_epoch(transformer,  optimizer, train_dataloader, loss_fn, data_config.source_pad_id, data_config.target_pad_id, DEVICE)\n",
    "    val_loss =  evaluate(transformer, val_dataloader, loss_fn, data_config.source_pad_id, data_config.target_pad_id, DEVICE)\n",
    "    pred_trgs, targets =  get_sequences(transformer, val_dataloader, data_config.source_pad_id, tgt_tokens_to_ids, max_len = 72, DEVICE = DEVICE)\n",
    "    if pred_trgs:\n",
    "        val_mapk = {f\"val_map@{k}\": mapk(targets, pred_trgs, k) for k in ks}\n",
    "        print('aa') \n",
    "        break\n",
    "        wandb.log({\"Epoch\": epoch, \"train_loss\": train_loss,\"val_loss\":val_loss, \"lr\" : optimizer.param_groups[0]['lr'], **val_mapk})\n",
    "    if config.scheduler :\n",
    "    # Step the scheduler based on its type\n",
    "        if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d35fd1e4-1dca-4ab3-9b77-30a0d823158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sifal.klioui\n"
     ]
    }
   ],
   "source": [
    "cd sifal.klioui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5841cda4-444c-45ab-aeb2-4ff32d4ca9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_to_old_ids_source file not availble, mapping is the same as the old on\n",
      "number of params: 106922688\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer = Seq2SeqTransformer(config.num_encoder_layers, config.num_decoder_layers, config.emb_size,\n",
    "                             config.nhead, data_config.source_vocab_size,\n",
    "                             data_config.target_vocab_size, config.ffn_hid_dim)\n",
    "\n",
    "print(f'number of params: {sum(p.numel() for p in transformer.parameters())}')\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    transformer = nn.DataParallel(transformer)\n",
    "\n",
    "transformer = transformer.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff586dc6-b5f8-49b6-b0e7-928c76886c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_to_old_ids_source file not availble, mapping is the same as the old on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msifalklioui\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sifal.klioui/wandb/run-20240502_145811-kbmpynt0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sifalklioui/PTF_SDP_D/runs/kbmpynt0' target=\"_blank\">royal-oath-53</a></strong> to <a href='https://wandb.ai/sifalklioui/PTF_SDP_D' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sifalklioui/PTF_SDP_D' target=\"_blank\">https://wandb.ai/sifalklioui/PTF_SDP_D</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sifalklioui/PTF_SDP_D/runs/kbmpynt0' target=\"_blank\">https://wandb.ai/sifalklioui/PTF_SDP_D/runs/kbmpynt0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/conda/Miniconda/envs/pytorch-2.2/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 106922688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|          | 0/208 [00:00<?, ?it/s]/public/conda/Miniconda/envs/pytorch-2.2/lib/python3.11/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/public/conda/Miniconda/envs/pytorch-2.2/lib/python3.11/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "train:   4%|▍         | 9/208 [00:22<08:26,  2.55s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">royal-oath-53</strong> at: <a href='https://wandb.ai/sifalklioui/PTF_SDP_D/runs/kbmpynt0' target=\"_blank\">https://wandb.ai/sifalklioui/PTF_SDP_D/runs/kbmpynt0</a><br/> View project at: <a href='https://wandb.ai/sifalklioui/PTF_SDP_D' target=\"_blank\">https://wandb.ai/sifalklioui/PTF_SDP_D</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240502_145811-kbmpynt0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m     13\u001b[0m   \u001b[38;5;66;03m# Set the project where this run will be logged\u001b[39;00m\n\u001b[1;32m     14\u001b[0m   project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPTF_SDP_D\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     15\u001b[0m   \u001b[38;5;66;03m# Track hyperparameters and run metadata\u001b[39;00m\n\u001b[1;32m     16\u001b[0m   config\u001b[38;5;241m=\u001b[39masdict(config))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     train_transformer(config, data_config, train_dataloader, val_dataloader)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     20\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(e)})\n",
      "Cell \u001b[0;32mIn[13], line 35\u001b[0m, in \u001b[0;36mtrain_transformer\u001b[0;34m(config, data_config, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# add wandb loss logging\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_train_epochs):\n\u001b[0;32m---> 35\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_epoch(transformer,  optimizer, train_dataloader, loss_fn, data_config\u001b[38;5;241m.\u001b[39msource_pad_id, data_config\u001b[38;5;241m.\u001b[39mtarget_pad_id, DEVICE)\n\u001b[1;32m     36\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m  evaluate(transformer, val_dataloader, loss_fn, data_config\u001b[38;5;241m.\u001b[39msource_pad_id, data_config\u001b[38;5;241m.\u001b[39mtarget_pad_id, DEVICE)\n\u001b[1;32m     37\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_loss,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:val_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m : optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[0;32m~/PatientTrajectoryForecasting/utils/train.py:295\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, train_dataloader, loss_fn, source_pad_id, target_pad_id, DEVICE)\u001b[0m\n\u001b[1;32m    293\u001b[0m _target_input_ids \u001b[38;5;241m=\u001b[39m target_input_ids[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    294\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), _target_input_ids\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 295\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    296\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    297\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/public/conda/Miniconda/envs/pytorch-2.2/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m/public/conda/Miniconda/envs/pytorch-2.2/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    config = Config()\n",
    "    data_config = DataConfig()\n",
    "    train_dataloader, val_dataloader, test_dataloader, src_tokens_to_ids, tgt_tokens_to_ids, _, data_and_properties  = get_data_loaders(**asdict(data_config))\n",
    "    \n",
    "    data_config.source_vocab_size = data_and_properties['embedding_size_source']\n",
    "    data_config.target_vocab_size = data_and_properties['embedding_size_target']\n",
    "    data_config.target_pad_id = tgt_tokens_to_ids['PAD']\n",
    "    data_config.source_pad_id = src_tokens_to_ids['PAD']\n",
    "    exit()\n",
    "    wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"PTF_SDP_D\", \n",
    "      # Track hyperparameters and run metadata\n",
    "      config=asdict(config))\n",
    "    try:\n",
    "        train_transformer(config, data_config, train_dataloader, val_dataloader, tgt_tokens_to_ids)\n",
    "    except Exception as e:\n",
    "        wandb.log({\"error\": str(e)})\n",
    "        raise e\n",
    "    finally:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747358f-9819-4e2c-8964-a2fde67fb108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
