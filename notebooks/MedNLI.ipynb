{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c9c904-8f07-401e-9aea-3074a81bdc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDNLI_PATH = '../physionet.org/files/mednli/1.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3ed177-d762-4e72-80ea-8fe5aab151db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8eae855-3db5-4a6a-b6e9-397a77b3800f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sifal.klioui/PatientTrajectoryForecasting\n"
     ]
    }
   ],
   "source": [
    "cd PatientTrajectoryForecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ac0700-468d-4225-8cff-a3bfa259591d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 20:54:52.434886: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-04 20:55:05.759502: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import wandb\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer, Trainer, TrainingArguments\n",
    "from utils.bert_classification import MosaicBertForSequenceClassification\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from torch.optim import AdamW\n",
    "from dataclasses import dataclass\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from utils.mednli import evaluate_model\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from typing import Optional, Tuple, Union\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.mednli import compute_metrics, evaluate_model, load_mednli, convert_to_dataset, NLIDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded8f977-3ef3-42ba-8af3-34b966e7b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_model (num_labels: int,\n",
    "    pretrained_model_name: str = 'bert-base-uncased',\n",
    "    model_config: Optional[dict] = None,\n",
    "    pretrained_checkpoint: Optional[str] = None,\n",
    "    alibi_starting_size = 1024):\n",
    "    \n",
    "    model_config, unused_kwargs = BertConfig.get_config_dict(model_config)\n",
    "    model_config.update(unused_kwargs)\n",
    "    \n",
    "    config, unused_kwargs = transformers.AutoConfig.from_pretrained(\n",
    "        pretrained_model_name, return_unused_kwargs=True, **model_config)\n",
    "    # This lets us use non-standard config fields (e.g. `starting_alibi_size`)\n",
    "    config.update(unused_kwargs)\n",
    "    config.num_labels = num_labels\n",
    "    \n",
    "    model = MosaicBertForSequenceClassification.from_pretrained(\n",
    "            pretrained_checkpoint=pretrained_checkpoint, config=config)\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80ddc1b8-fa23-4e33-ab04-fd5653af3f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mstep_0\u001b[0m/  \u001b[01;34mstep_10000\u001b[0m/  \u001b[01;34mstep_20000\u001b[0m/  \u001b[01;34mstep_30000\u001b[0m/  \u001b[01;34mstep_40000\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls ../bert_mimic_model_512/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564e70c2-ea91-4aa8-bd64-5e213096a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = os.path.join('..', 'bert_mimic_model_512/step_40000', 'pytorch_model.bin')\n",
    "\n",
    "num_labels = 3\n",
    "pretrained_model_name = 'mosaicml/mosaic-bert-base-seqlen-512'\n",
    "model_config = 'mosaicml/mosaic-bert-base-seqlen-512'\n",
    "pretrained_checkpoint = os.path.abspath(relative_path)\n",
    "\n",
    "\n",
    "get_model = partial(_get_model, num_labels=num_labels, pretrained_model_name=pretrained_model_name, model_config=model_config, pretrained_checkpoint=pretrained_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ef30cdb-1bc9-4ee3-8c4e-4548be53738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All variants use the same tokenizer :))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17554b74-2c9a-464c-aade-497edbf2ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MedNLI dataset\n",
    "train_data = load_mednli(os.path.join(MEDNLI_PATH,'mli_train_v1.jsonl'))\n",
    "dev_data = load_mednli(os.path.join(MEDNLI_PATH,'mli_dev_v1.jsonl'))\n",
    "test_data = load_mednli(os.path.join(MEDNLI_PATH,'mli_test_v1.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec03e3a5-7b6a-4aba-99a5-55d8fa97cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in [ train_data, dev_data, test_data]:\n",
    "    assert sorted(list(set([item['gold_label'] for item in partition]))) == ['contradiction', 'entailment', 'neutral'], 'the are some issues with the labels in you dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cf4c802-1dfc-48ce-b0e6-5234045fa7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9db4da6303452995c3103830af2343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d89baa59a24abeaa955f5330062b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec729cf3a6549038bfb05160427bda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1422 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = convert_to_dataset(train_data)\n",
    "dev_dataset = convert_to_dataset(dev_data)\n",
    "test_dataset = convert_to_dataset(test_data)\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': dev_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples, max_length = 512):\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'], return_tensors ='pt', padding='max_length', max_length = max_length)\n",
    "\n",
    "encoded_dataset = dataset_dict.map(preprocess_function, batched=True, remove_columns=['premise', 'hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f5ce99a-7fa5-43cf-8520-639b67cf6f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataset variable is named 'dataset'\n",
    "train_dataset = NLIDataset(encoded_dataset['train'])\n",
    "validation_dataset = NLIDataset(encoded_dataset['validation'])\n",
    "test_dataset = NLIDataset(encoded_dataset['test'])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=128)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "445cf66b-429e-4eb2-807b-cd042dc662ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    train_file : str = '/scratch/sifal.klioui/notes_v2/notes.txt'\n",
    "    max_seq_length : str = 512\n",
    "\n",
    "args = Args()\n",
    "\n",
    "args.per_device_train_batch_size = 16\n",
    "args.per_device_eval_batch_size = 256\n",
    "\n",
    "args.learning_rate = 1e-5\n",
    "args.beta1 = 0.9\n",
    "args.beta2 = 0.98\n",
    "args.eps = 1e-06\n",
    "args.weight_decay =  1e-6\n",
    "args.num_train_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a28e01a-a75a-46a8-b57c-886730af2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_optimizer(args):\n",
    "    model = get_model()\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, betas =(args.beta1, args.beta2), eps=args.eps, weight_decay = args.weight_decay)\n",
    "\n",
    "    #scheduler = ReduceLROnPlateau(optimizer, 'min', min_lr = args.learning_rate *0.02, patience=2, factor=0.7)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5ad67c2-3f73-490c-b839-5517dbdcca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e97a886-36bd-4031-a69c-21b0955f8d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sifal.klioui/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/sifal.klioui/PatientTrajectoryForecasting/utils/bert_layers_mosa.py:177: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Found these missing keys in the checkpoint: bert.pooler.dense.weight, bert.pooler.dense.bias, classifier.weight, classifier.bias\n",
      "the number of which is equal to 4\n",
      "Found these unexpected keys in the checkpoint: cls.predictions.transform.dense.weight, cls.predictions.transform.dense.bias, cls.predictions.transform.LayerNorm.weight, cls.predictions.transform.LayerNorm.bias, cls.predictions.decoder.weight, cls.predictions.decoder.bias\n",
      "the number of which is equal to 6\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, optimizer = get_model_and_optimizer(args)\n",
    "model.to(DEVICE)\n",
    "total_steps = args.num_train_epochs * len(train_dataloader) \n",
    "#lr_s = transformers.get_wsd_schedule(optimizer, round(total_steps *0.06), 0, total_steps - round((total_steps*0.06)), 0.02 * args.learning_rate)\n",
    "#scheduler = ReduceLROnPlateau(optimizer, 'min', min_lr = args.learning_rate *0.02, patience=2, factor=0.7, threshold = 0.04)\n",
    "\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db18a67-6a54-4f14-a638-717a56b7477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [02:04<00:00,  2.81it/s, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean epoch loss = 1.0999709201334549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:04<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.34838709677419355, 'f1': 0.34838709677419355, 'precision': 0.34838709677419355, 'recall': 0.34838709677419355} validation 1.0889542427929966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [01:44<00:00,  3.35it/s, loss=1.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean epoch loss = 1.0976798948738988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:04<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.31827956989247314, 'f1': 0.31827956989247314, 'precision': 0.31827956989247314, 'recall': 0.31827956989247314} validation 1.1119456941431218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351/351 [01:44<00:00,  3.35it/s, loss=1.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean epoch loss = 1.1044039923241336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:04<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.3311827956989247, 'f1': 0.3311827956989247, 'precision': 0.3311827956989247, 'recall': 0.3311827956989247} validation 1.0987502228129993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 152/351 [00:45<00:59,  3.36it/s, loss=1.06]"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.num_train_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_dataloader, position=0, leave=True)\n",
    "    sum_loss = 0 \n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        # Move batch to the device\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        token_type_ids = batch['token_type_ids'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        # Forward pass\n",
    "        outputs = model.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = model.dropout(pooled_output)\n",
    "        logits = model.classifier(pooled_output)\n",
    "        \n",
    "        loss = criterion(logits.view(-1, model.num_labels), labels.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #r_scheduler.step()\n",
    "        # Update progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        sum_loss += loss.item()\n",
    "    print(f'mean epoch loss = {sum_loss/len(train_dataloader)}')\n",
    "    \n",
    "    metrics, val_loss = evaluate_model(model, validation_dataloader, DEVICE, criterion)\n",
    "    print(metrics,'validation', val_loss)\n",
    "    #scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f6053-0a21-495e-9c0b-8ef227f0fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, test_loss = evaluate_model(model, test_dataloader, DEVICE, criterion)\n",
    "print(metrics,'test', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ea5bc-97bb-4923-98c3-5dd056c4e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(args.num_train_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_dataloader, position=0, leave=True)\n",
    "    sum_loss = 0 \n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        # Move batch to the device\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        token_type_ids = batch['token_type_ids'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #r_scheduler.step()\n",
    "        # Update progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        sum_loss += loss.item()\n",
    "    print(f'mean epoch loss = {sum_loss/len(train_dataloader)}')\n",
    "    metrics, val_loss = evaluate_model(model, validation_dataloader, DEVICE, criterion)\n",
    "    print(metrics,'validation', val_loss)\n",
    "    #scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d25b45-e573-4142-a4ba-ffe3c5fa066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, test_loss = evaluate_model(model, test_dataloader, DEVICE, criterion)\n",
    "print(metrics,'test', test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dude",
   "language": "python",
   "name": "mimibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
