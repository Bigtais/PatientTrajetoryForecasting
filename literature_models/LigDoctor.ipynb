{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836cd974-152b-4b90-8d0d-0ee0974d5670",
   "metadata": {},
   "source": [
    "# Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0057cadb-6342-4589-931b-2b5d1b66a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from tqdm import tqdm \n",
    "from datasets import  load_from_disk\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PatientTrajectoryForecasting.utils.utils import (\n",
    "    load_data,\n",
    "    get_paths,\n",
    ")\n",
    "\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1881181-6e7f-47a4-bdac-14edee408b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, source_vocab_size, embedding_dim = 714, hidden_dim = 714, n_layers = 1, batch_first = True, bidirectional = True):\n",
    "        super().__init__()\n",
    " \n",
    "        self.embedding = nn.Embedding(source_vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first = batch_first, bidirectional = bidirectional)\n",
    "        self.encode = nn.Linear(512, 1)\n",
    "        self.fuse_directions = nn.Linear(714 * (1 + 1 * bidirectional) , hidden_dim)\n",
    "        self.hidden_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    " \n",
    "    def forward(self, input_batch):\n",
    "        embed = self.embedding(input_batch)\n",
    "        outputs, hidden = self.rnn(embed)\n",
    "        encoded = self.encode(outputs.transpose(1, 2)).squeeze(-1)\n",
    "        fused_dirs = self.fuse_directions(encoded)\n",
    "        outs = torch.nn.functional.softmax(self.hidden_layer(fused_dirs), dim = -1)\n",
    " \n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84b2411-e841-45f7-a273-5174534e7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForcastWithNotes(Dataset):\n",
    "    def __init__(self, source_sequences, target_sequences, hospital_ids, tokenized_notes):\n",
    "        self.source_sequences = source_sequences\n",
    "        self.target_sequences = target_sequences\n",
    "        self.hospital_ids = hospital_ids\n",
    "        self.tokenized_notes = load_from_disk(tokenized_notes)\n",
    "    def __len__(self):\n",
    "        return len(self.source_sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        hospital_ids = self.hospital_ids[idx]\n",
    "        hospital_ids_lens = len(hospital_ids)\n",
    "\n",
    "        return  {'source_sequences':torch.tensor(self.source_sequences[idx]),\n",
    "                 'target_sequences': torch.tensor(self.target_sequences[idx]),\n",
    "                 'tokenized_notes':self.tokenized_notes[hospital_ids],\n",
    "                 'hospital_ids_lens': hospital_ids_lens}\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    source_sequences = [item['source_sequences'] for item in batch]\n",
    "    target_sequences = []\n",
    "    for item in batch:\n",
    "        one_hot = torch.zeros(714)\n",
    "        one_hot[item['target_sequences']] = 1\n",
    "        target_sequences.append(one_hot)\n",
    "        \n",
    "    source_sequences = torch.stack(source_sequences, dim=0)\n",
    "    \n",
    "    target_sequences = torch.stack(target_sequences, dim=0)\n",
    "\n",
    "    return {\n",
    "        'source_sequences': source_sequences,\n",
    "        'target_sequences': target_sequences,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2fbecb2-3257-4f47-bb76-528af5a77fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    source_sequences = [item['source_sequences'] for item in batch]\n",
    "    target_sequences = []\n",
    "    for item in batch:\n",
    "        one_hot = torch.zeros(714)\n",
    "        one_hot[item['target_sequences']] = 1\n",
    "        target_sequences.append(one_hot)\n",
    "        \n",
    "    source_sequences = torch.stack(source_sequences, dim=0)\n",
    "    \n",
    "    target_sequences = torch.stack(target_sequences, dim=0)\n",
    "\n",
    "    return {\n",
    "        'source_sequences': source_sequences,\n",
    "        'target_sequences': target_sequences,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79afdb0-7cc3-4146-b929-7b22a8b10485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn_test(batch):\n",
    "    source_sequences = [item['source_sequences'] for item in batch]\n",
    "    target_sequences = [item['target_sequences'] for item in batch]\n",
    "    \n",
    "    source_sequences = torch.stack(source_sequences, dim=0)\n",
    "    target_sequences = torch.stack(target_sequences, dim=0)\n",
    "\n",
    "    return {\n",
    "        'source_sequences': source_sequences,\n",
    "        'target_sequences': target_sequences,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71731f74-65d0-4c2c-afe6-1f977ad0b09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_to_new_ids_source file not availble, mapping is the same as the old one\n"
     ]
    }
   ],
   "source": [
    "with open('PatientTrajectoryForecasting/paths.yaml', 'r') as file:\n",
    "        path_config = yaml.safe_load(file)\n",
    "\n",
    "train_data_path = get_paths(path_config,\n",
    "                        'SDP',\n",
    "                        False,\n",
    "                        False,\n",
    "                        train = True,\n",
    "                        processed_data = True,\n",
    "                        with_notes = True)\n",
    "\n",
    "\n",
    "source_sequences, target_sequences, source_tokens_to_ids, target_tokens_to_ids, _, __, hospital_ids_source = load_data(train_data_path['processed_data_path'],\n",
    "                                                                                                                   processed_data = True, reindexed = True)\n",
    "# Load the datasets\n",
    "train_dataset = torch.load('final_dataset/train_dataset.pth')\n",
    "val_dataset = torch.load('final_dataset/val_dataset.pth')\n",
    "test_dataset = torch.load('final_dataset/test_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e462553a-cc91-4701-8a4e-4ac684dfbe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(source_tokens_to_ids, target_tokens_to_ids, embedding_dim = 714, hidden_dim = 714, n_layers = 1):\n",
    "    # Define the required dimensions and hyper parameters\n",
    "   \n",
    " \n",
    "    # Instanciate the models\n",
    "    model = Encoder(len(source_tokens_to_ids), embedding_dim, hidden_dim, n_layers=n_layers )\n",
    " \n",
    "    model = model.to(DEVICE)\n",
    " \n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adadelta(model.parameters())\n",
    "    criterion = torch.nn.BCELoss()\n",
    " \n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f95ffb-8efb-4a4e-9cde-342b4a7fe850",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 512\n",
    "val_batch_size = train_batch_size * 2\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                                  shuffle = True,\n",
    "                                  batch_size = train_batch_size,\n",
    "                                  num_workers = int(os.environ[\"SLURM_CPUS_PER_TASK\"]),\n",
    "                                  pin_memory = True,\n",
    "                                  collate_fn = custom_collate_fn)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            shuffle = False,\n",
    "                            batch_size = val_batch_size,\n",
    "                            num_workers = int(os.environ[\"SLURM_CPUS_PER_TASK\"]),\n",
    "                            pin_memory = True,\n",
    "                            collate_fn = custom_collate_fn)\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             shuffle = False,\n",
    "                             batch_size = val_batch_size,\n",
    "                             num_workers = int(os.environ[\"SLURM_CPUS_PER_TASK\"]),\n",
    "                             pin_memory = True,\n",
    "                             collate_fn = custom_collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "211565d6-60c1-45d0-a23e-2fec4c223bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f733a0f-6b02-4c67-8f28-be4c172fc70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iterator, valid_iterator, source_tokens_to_ids, target_tokens_to_ids, epochs=10, patience=10):\n",
    "    model, optimizer, criterion = create_model(source_tokens_to_ids, target_tokens_to_ids)\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    " \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        pbar = tqdm(total=len(train_iterator), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}', unit=' batches', ncols=200)\n",
    " \n",
    "        training_loss = []\n",
    "        # set training mode\n",
    "        model.train()\n",
    " \n",
    "        # Loop through the training batch\n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            # Get the source and target tokens\n",
    "            src = batch['source_sequences'].to(DEVICE)\n",
    "            trg = batch['target_sequences'].to(DEVICE)\n",
    " \n",
    "            optimizer.zero_grad()\n",
    " \n",
    "            # Forward pass\n",
    "            output = model(src)\n",
    " \n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, trg)\n",
    " \n",
    "            # back propagation\n",
    "            loss.backward()\n",
    " \n",
    "            optimizer.step()\n",
    " \n",
    "            training_loss.append(loss.item())\n",
    " \n",
    "            pbar.set_postfix(\n",
    "                epoch=f\" {epoch}, train loss= {round(sum(training_loss) / len(training_loss), 4)}\", refresh=True)\n",
    "            pbar.update()\n",
    " \n",
    "        with torch.inference_mode():\n",
    "            # Set the model to eval\n",
    "            model.eval()\n",
    " \n",
    "            validation_loss = []\n",
    " \n",
    "            # Loop through the validation batch\n",
    "            for i, batch in enumerate(valid_iterator):\n",
    "                src = batch['source_sequences'].to(DEVICE)\n",
    "                trg = batch['target_sequences'].to(DEVICE)\n",
    " \n",
    "                # Forward pass\n",
    "                output = model(src)\n",
    "                \n",
    "                # Calculate Loss\n",
    "                loss = criterion(output, trg)\n",
    " \n",
    "                validation_loss.append(loss.item())\n",
    " \n",
    "        avg_val_loss = sum(validation_loss) / len(validation_loss)\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            epoch=f\" {epoch}, train loss= {round(sum(training_loss) / len(training_loss), 4)}, val loss= {round(avg_val_loss, 4)}\",\n",
    "            refresh=False)\n",
    "        pbar.close()\n",
    "        \n",
    "        # Check for improvement\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Early stopping condition\n",
    "        if epochs_without_improvement == patience:\n",
    "            print(f\"Stopping early after {epoch} epochs due to no improvement in validation loss.\")\n",
    "            break\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9b2ec373-1578-4d9c-bf0a-dddcebb788fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:19<00:00,  2.63 batches/s, epoch=1, train loss= 0.1387, val loss= 0.1427]                                                                                                    \n",
      "100%|██████████| 52/52 [00:20<00:00,  2.59 batches/s, epoch=2, train loss= 0.1292, val loss= 0.1249]                                                                                                    \n",
      "100%|██████████| 52/52 [00:20<00:00,  2.56 batches/s, epoch=3, train loss= 0.1244, val loss= 0.1259]                                                                                                    \n",
      "100%|██████████| 52/52 [00:20<00:00,  2.58 batches/s, epoch=4, train loss= 0.1244, val loss= 0.1275]                                                                                                    \n",
      "100%|██████████| 52/52 [00:20<00:00,  2.60 batches/s, epoch=5, train loss= 0.1237, val loss= 0.1279]                                                                                                    \n",
      "100%|██████████| 52/52 [00:19<00:00,  2.64 batches/s, epoch=6, train loss= 0.1239, val loss= 0.1249]                                                                                                    \n",
      "100%|██████████| 52/52 [00:19<00:00,  2.62 batches/s, epoch=7, train loss= 0.1238, val loss= 0.13]                                                                                                      \n",
      "100%|██████████| 52/52 [00:19<00:00,  2.70 batches/s, epoch=8, train loss= 0.1232, val loss= 0.1247]                                                                                                    \n",
      "100%|██████████| 52/52 [00:19<00:00,  2.66 batches/s, epoch=9, train loss= 0.1225, val loss= 0.1295]                                                                                                    \n",
      "100%|██████████| 52/52 [00:19<00:00,  2.74 batches/s, epoch=10, train loss= 0.1229, val loss= 0.1262]                                                                                                   \n",
      "100%|██████████| 52/52 [00:19<00:00,  2.71 batches/s, epoch=11, train loss= 0.1228, val loss= 0.1305]                                                                                                   \n",
      "100%|██████████| 52/52 [00:19<00:00,  2.70 batches/s, epoch=12, train loss= 0.1222, val loss= 0.129]                                                                                                    \n",
      "100%|██████████| 52/52 [00:19<00:00,  2.70 batches/s, epoch=13, train loss= 0.1229, val loss= 0.1302]                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early after 13 epochs due to no improvement in validation loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train(train_dataloader, val_dataloader, source_tokens_to_ids, target_tokens_to_ids, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "389613f7-fe86-4ee2-a703-290b2b4eeee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "50eee89a-17ed-4b20-b954-ac79de81c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences_lig_doctor(model, dataloader : torch.utils.data.dataloader.DataLoader,\n",
    "                            target_tokens_to_ids : Dict[str, int] =  None, max_len : int = 150,\n",
    "                            DEVICE : str ='cuda:0'):\n",
    "    model.eval()\n",
    "    pred_trgs = []\n",
    "    targets = []\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(test_dataloader, desc='scoring'):\n",
    "            source_input_ids, target_input_ids = batch['source_sequences'].to(DEVICE),batch['target_sequences'].to(DEVICE)\n",
    "            output = model(source_input_ids)\n",
    "            preds = torch.topk(output, k = max_len).indices\n",
    "            for i in range(target_input_ids.size(0)):\n",
    "                filtred_preds = preds[i][preds[i]>4]\n",
    "                filtred_targets = target_input_ids[i][:len(filtred_preds)]\n",
    "                pred_trgs.extend([filtred_preds.tolist()])\n",
    "                targets.extend([filtred_targets.tolist()])\n",
    "    return pred_trgs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ac64aeb3-818f-450b-84f5-ca3d1e6a8097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring: 100%|██████████| 8/8 [00:03<00:00,  2.48it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_trgs, targets = get_sequences_lig_doctor(model, test_dataloader, target_tokens_to_ids, max_len = 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "77999498-88bc-48e4-8588-d61603cf371b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_map@20': 0.21429694482427702, 'test_map@40': 0.20762199508821036, 'test_map@60': 0.20054600616706678} {'test_recall@20': 0.11025470376871768, 'test_recall@40': 0.19890656898731335, 'test_recall@60': 0.2883833203196874}\n"
     ]
    }
   ],
   "source": [
    "from utils.eval import mapk, recallTop\n",
    "\n",
    "\n",
    "ks = [20, 40, 60]\n",
    "\n",
    "test_mapk = {f\"test_map@{k}\": mapk(targets, pred_trgs, k) for k in ks}\n",
    "test_recallk = {f\"test_recall@{k}\": recallTop(targets, pred_trgs, rank = [k])[0] for k in ks}\n",
    "print(test_mapk, test_recallk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b42ff1c-1ac7-4a42-bc50-a04b415de89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring:   0%|          | 0/8 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 96\n",
    "\n",
    "model.eval()\n",
    "pred_trgs = []\n",
    "targets = []\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(test_dataloader, desc='scoring'):\n",
    "        source_input_ids, target_input_ids = batch['source_sequences'].to(DEVICE),batch['target_sequences'].to(DEVICE)\n",
    "        output = model(source_input_ids)\n",
    "        preds = torch.topk(output, k = max_len).indices\n",
    "        for i in range(test_dataloader.batch_size):\n",
    "            filtred_preds = preds[i][preds[i]>4]\n",
    "            filtred_targets = target_input_ids[i][:len(filtred_preds)]\n",
    "            pred_trgs.extend(filtred_preds)\n",
    "            targets.extend(filtred_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2980ddf-8ae3-40c6-b15e-f8b4e766ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.topk(output, k = max_len).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "46291d79-b53b-475a-bc1c-fc16853d4631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader.batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2627e59c-8fab-4313-8948-59fb93c48c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "37992d78-2d2d-4274-a436-43dd01c6a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test_dataloader.batch_size):\n",
    "    filtred_preds = preds[i][preds[i]>4]\n",
    "    filtred_targets = target_input_ids[i][:len(filtred_preds)]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd0e95a6-ac8f-4e40-b53f-7b7a9849596f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtred_targets) == len(filtred_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7f64e-f049-4b3e-9336-0955a0a32990",
   "metadata": {},
   "outputs": [],
   "source": [
    "off = []\n",
    "off.extend("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9650591e-d243-4205-80a3-c8315147ee51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 96])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7fb0587e-6e06-408f-90ea-ee0df858a5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds[i][preds[i]>4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978053c8-0297-423f-af24-5af41f165cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_input_ids[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebe043-2ecb-4304-9a50-e0a16bd13cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78b665b4-d640-4c19-9be1-9e8707130e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0e5163c-5d85-4fe5-a655-8c378e45dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_stats(targets: List[List[int]], seq_len : int = 96, ks : List[int] = [20, 40, 60], num_runs_avg : int = 5):\n",
    "    \"\"\"\n",
    "    Returns the average MAP@k and Recall@k scores for a random forecasting model.\n",
    "\n",
    "    Args:\n",
    "        targets (List[List[int]]): The list of target sequences.\n",
    "        seq_len (int, optional): The length of the forecasted sequence. Defaults to 96.\n",
    "        ks (List[int], optional): The list of k values for MAP@k and Recall@k. Defaults to [20, 40, 60].\n",
    "        num_runs_avg (int, optional): The number of runs to average the results over. Defaults to 5.\n",
    "    Returns:\n",
    "        Dict[str, float], Dict[str, float]: The average MAP@k and Recall@k scores.\n",
    "    \"\"\"\n",
    "    # targets = [concated_dt[i]['target_sequences'].numpy().tolist() for i in range(len(concated_dt))]\n",
    "    unique_targets = list(set([item for sublist in targets for item in sublist]))\n",
    "    \n",
    "    cumulative_mapk = {f\"test_map@{k}\": 0.0 for k in ks}\n",
    "    cumulative_recallk = {f\"test_recall@{k}\": 0.0 for k in ks}\n",
    "    \n",
    "    for _ in range(num_runs_avg):\n",
    "        \n",
    "        forecasted = [np.random.choice(unique_targets, size=seq_len, replace=False).tolist() for _ in range(len(targets))]\n",
    "    \n",
    "        run_mapk = {f\"test_map@{k}\": mapk(targets, forecasted, k) for k in ks}\n",
    "        run_recallk = {f\"test_recall@{k}\": recallTop(targets, forecasted, rank=[k])[0] for k in ks}\n",
    "    \n",
    "        # Accumulate results\n",
    "        for k in ks:\n",
    "            cumulative_mapk[f\"test_map@{k}\"] += run_mapk[f\"test_map@{k}\"]\n",
    "            cumulative_recallk[f\"test_recall@{k}\"] += run_recallk[f\"test_recall@{k}\"]\n",
    "    \n",
    "    # Compute average results\n",
    "    average_mapk = {f\"test_map@{k}\": cumulative_mapk[f\"test_map@{k}\"] / num_runs_avg for k in ks}\n",
    "    average_recallk = {f\"test_recall@{k}\": cumulative_recallk[f\"test_recall@{k}\"] / num_runs_avg for k in ks}\n",
    "\n",
    "    return average_mapk, average_recallk    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8e2ff25-ef5a-48c0-8a9d-359ab9f3777e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'test_map@20': 0.05347963114250881,\n",
       "  'test_map@40': 0.0573658107319462,\n",
       "  'test_map@60': 0.05544175641942543},\n",
       " {'test_recall@20': 0.029036955706191657,\n",
       "  'test_recall@40': 0.05811648229377542,\n",
       "  'test_recall@60': 0.08716030403904587})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_stats(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "727eb57f-941d-4802-9412-f4cfdfc9c734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 96)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(forecasted[0]), len(set(forecasted[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c127c53-e0b2-4edd-aff5-78fec982c707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dude",
   "language": "python",
   "name": "mimibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
