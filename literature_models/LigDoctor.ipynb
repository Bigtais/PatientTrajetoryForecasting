{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836cd974-152b-4b90-8d0d-0ee0974d5670",
   "metadata": {},
   "source": [
    "# Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0057cadb-6342-4589-931b-2b5d1b66a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from tqdm import tqdm \n",
    "from datasets import  load_from_disk\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PatientTrajectoryForecasting.utils.utils import (\n",
    "    load_data,\n",
    "    get_paths,\n",
    ")\n",
    "\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1881181-6e7f-47a4-bdac-14edee408b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, source_vocab_size, embedding_dim = 714, hidden_dim = 714, n_layers = 1, batch_first = True, bidirectional = True):\n",
    "        super().__init__()\n",
    " \n",
    "        self.embedding = nn.Embedding(source_vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first = batch_first, bidirectional = bidirectional)\n",
    "        self.encode = nn.Linear(512, 1)\n",
    "        self.fuse_directions = nn.Linear(714 * (1 + 1 * bidirectional) , hidden_dim)\n",
    "        self.hidden_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    " \n",
    "    def forward(self, input_batch):\n",
    "        embed = self.embedding(input_batch)\n",
    "        outputs, hidden = self.rnn(embed)\n",
    "        encoded = self.encode(outputs.transpose(1, 2)).squeeze(-1)\n",
    "        fused_dirs = self.fuse_directions(encoded)\n",
    "        outs = torch.nn.functional.softmax(self.hidden_layer(fused_dirs), dim = -1)\n",
    " \n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84b2411-e841-45f7-a273-5174534e7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForcastWithNotes(Dataset):\n",
    "    def __init__(self, source_sequences, target_sequences, hospital_ids, tokenized_notes):\n",
    "        self.source_sequences = source_sequences\n",
    "        self.target_sequences = target_sequences\n",
    "        self.hospital_ids = hospital_ids\n",
    "        self.tokenized_notes = load_from_disk(tokenized_notes)\n",
    "    def __len__(self):\n",
    "        return len(self.source_sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        hospital_ids = self.hospital_ids[idx]\n",
    "        hospital_ids_lens = len(hospital_ids)\n",
    "\n",
    "        return  {'source_sequences':torch.tensor(self.source_sequences[idx]),\n",
    "                 'target_sequences': torch.tensor(self.target_sequences[idx]),\n",
    "                 'tokenized_notes':self.tokenized_notes[hospital_ids],\n",
    "                 'hospital_ids_lens': hospital_ids_lens}\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    source_sequences = [item['source_sequences'] for item in batch]\n",
    "    target_sequences = []\n",
    "    for item in batch:\n",
    "        one_hot = torch.zeros(714)\n",
    "        one_hot[item['target_sequences']] = 1\n",
    "        target_sequences.append(one_hot)\n",
    "        \n",
    "    source_sequences = torch.stack(source_sequences, dim=0)\n",
    "    \n",
    "    target_sequences = torch.stack(target_sequences, dim=0)\n",
    "\n",
    "    return {\n",
    "        'source_sequences': source_sequences,\n",
    "        'target_sequences': target_sequences,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2fbecb2-3257-4f47-bb76-528af5a77fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    source_sequences = [item['source_sequences'] for item in batch]\n",
    "    target_sequences = []\n",
    "    for item in batch:\n",
    "        one_hot = torch.zeros(714)\n",
    "        one_hot[item['target_sequences']] = 1\n",
    "        target_sequences.append(one_hot)\n",
    "        \n",
    "    source_sequences = torch.stack(source_sequences, dim=0)\n",
    "    \n",
    "    target_sequences = torch.stack(target_sequences, dim=0)\n",
    "\n",
    "    return {\n",
    "        'source_sequences': source_sequences,\n",
    "        'target_sequences': target_sequences,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79afdb0-7cc3-4146-b929-7b22a8b10485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn_test(batch):\n",
    "    source_sequences = [item['source_sequences'] for item in batch]\n",
    "    target_sequences = [item['target_sequences'] for item in batch]\n",
    "    \n",
    "    source_sequences = torch.stack(source_sequences, dim=0)\n",
    "    target_sequences = torch.stack(target_sequences, dim=0)\n",
    "\n",
    "    return {\n",
    "        'source_sequences': source_sequences,\n",
    "        'target_sequences': target_sequences,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71731f74-65d0-4c2c-afe6-1f977ad0b09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_to_new_ids_source file not availble, mapping is the same as the old one\n"
     ]
    }
   ],
   "source": [
    "with open('PatientTrajectoryForecasting/paths.yaml', 'r') as file:\n",
    "        path_config = yaml.safe_load(file)\n",
    "\n",
    "train_data_path = get_paths(path_config,\n",
    "                        'SDP',\n",
    "                        False,\n",
    "                        False,\n",
    "                        train = True,\n",
    "                        processed_data = True,\n",
    "                        with_notes = True)\n",
    "\n",
    "\n",
    "source_sequences, target_sequences, source_tokens_to_ids, target_tokens_to_ids, _, __, hospital_ids_source = load_data(train_data_path['processed_data_path'],\n",
    "                                                                                                                   processed_data = True, reindexed = True)\n",
    "# Load the datasets\n",
    "train_dataset = torch.load('final_dataset/train_dataset.pth')\n",
    "val_dataset = torch.load('final_dataset/val_dataset.pth')\n",
    "test_dataset = torch.load('final_dataset/test_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e462553a-cc91-4701-8a4e-4ac684dfbe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(source_tokens_to_ids, target_tokens_to_ids, embedding_dim = 714, hidden_dim = 714, n_layers = 1):\n",
    "    # Define the required dimensions and hyper parameters\n",
    "   \n",
    " \n",
    "    # Instanciate the models\n",
    "    model = Encoder(len(source_tokens_to_ids), embedding_dim, hidden_dim, n_layers=n_layers )\n",
    " \n",
    "    model = model.to(DEVICE)\n",
    " \n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adadelta(model.parameters())\n",
    "    criterion = torch.nn.BCELoss()\n",
    " \n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f95ffb-8efb-4a4e-9cde-342b4a7fe850",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 512\n",
    "val_batch_size = train_batch_size * 2\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                                  shuffle = True,\n",
    "                                  batch_size = train_batch_size,\n",
    "                                  num_workers = int(os.environ[\"SLURM_CPUS_PER_TASK\"]),\n",
    "                                  pin_memory = True,\n",
    "                                  collate_fn = custom_collate_fn)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            shuffle = False,\n",
    "                            batch_size = val_batch_size,\n",
    "                            num_workers = int(os.environ[\"SLURM_CPUS_PER_TASK\"]),\n",
    "                            pin_memory = True,\n",
    "                            collate_fn = custom_collate_fn)\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             shuffle = False,\n",
    "                             batch_size = val_batch_size,\n",
    "                             num_workers = int(os.environ[\"SLURM_CPUS_PER_TASK\"]),\n",
    "                             pin_memory = True,\n",
    "                             collate_fn = custom_collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "211565d6-60c1-45d0-a23e-2fec4c223bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f733a0f-6b02-4c67-8f28-be4c172fc70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iterator, valid_iterator, source_tokens_to_ids, target_tokens_to_ids, epochs=10, patience=10):\n",
    "    \n",
    "    model, optimizer, criterion = create_model(source_tokens_to_ids, target_tokens_to_ids)\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    " \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        pbar = tqdm(total=len(train_iterator), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}', unit=' batches', ncols=200)\n",
    " \n",
    "        training_loss = []\n",
    "        # set training mode\n",
    "        model.train()\n",
    " \n",
    "        # Loop through the training batch\n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            # Get the source and target tokens\n",
    "            src = batch['source_sequences'].to(DEVICE)\n",
    "            trg = batch['target_sequences'].to(DEVICE)\n",
    " \n",
    "            optimizer.zero_grad()\n",
    " \n",
    "            # Forward pass\n",
    "            output = model(src)\n",
    " \n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, trg)\n",
    " \n",
    "            # back propagation\n",
    "            loss.backward()\n",
    " \n",
    "            optimizer.step()\n",
    " \n",
    "            training_loss.append(loss.item())\n",
    " \n",
    "            pbar.set_postfix(\n",
    "                epoch=f\" {epoch}, train loss= {round(sum(training_loss) / len(training_loss), 4)}\", refresh=True)\n",
    "            pbar.update()\n",
    "\n",
    "        pred_trgs, targets = get_sequences_lig_doctor(model, test_dataloader, max_len = 96)\n",
    "\n",
    "        test_mapk = {f\"test_map@{k}\": mapk(targets, pred_trgs, k) for k in ks}\n",
    "        test_recallk = {f\"test_recall@{k}\": recallTop(targets, pred_trgs, rank = [k])[0] for k in ks}\n",
    " \n",
    "    return test_mapk, test_recallk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b2ec373-1578-4d9c-bf0a-dddcebb788fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 51/52 [00:20<00:00,  2.70 batches/s, epoch=1, train loss= 0.1376]                                                                                                                      \n",
      "scoring:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "scoring:  12%|█▎        | 1/8 [00:03<00:23,  3.36s/it]\u001b[A\n",
      "scoring:  25%|██▌       | 2/8 [00:03<00:09,  1.59s/it]\u001b[A\n",
      "scoring:  38%|███▊      | 3/8 [00:04<00:05,  1.02s/it]\u001b[A\n",
      "scoring:  50%|█████     | 4/8 [00:04<00:03,  1.33it/s]\u001b[A\n",
      "scoring:  62%|██████▎   | 5/8 [00:04<00:01,  1.65it/s]\u001b[A\n",
      "scoring:  75%|███████▌  | 6/8 [00:05<00:01,  1.93it/s]\u001b[A\n",
      "scoring:  88%|████████▊ | 7/8 [00:05<00:00,  2.18it/s]\u001b[A\n",
      "scoring: 100%|██████████| 8/8 [00:05<00:00,  1.42it/s]\u001b[A\n",
      "100%|██████████| 52/52 [00:29<00:00,  1.75 batches/s, epoch=1, train loss= 0.1376]                                                                                                                      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'test_map@20': 0.29729965586703744,\n",
       "  'test_map@40': 0.268215048857014,\n",
       "  'test_map@60': 0.24991464844147562},\n",
       " {'test_recall@20': 0.18689054511057224,\n",
       "  'test_recall@40': 0.29336155234242206,\n",
       "  'test_recall@60': 0.35899976201540224})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(train_dataloader, val_dataloader, source_tokens_to_ids, target_tokens_to_ids, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "389613f7-fe86-4ee2-a703-290b2b4eeee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50eee89a-17ed-4b20-b954-ac79de81c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences_lig_doctor(model, dataloader : torch.utils.data.dataloader.DataLoader,\n",
    "                             max_len : int = 150,\n",
    "                             DEVICE : str ='cuda:0'):\n",
    "    model.eval()\n",
    "    pred_trgs = []\n",
    "    targets = []\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(dataloader, desc='scoring'):\n",
    "            source_input_ids, target_input_ids = batch['source_sequences'].to(DEVICE),batch['target_sequences'].to(DEVICE)\n",
    "            output = model(source_input_ids)\n",
    "            preds = torch.topk(output, k = max_len).indices\n",
    "            for i in range(target_input_ids.size(0)):\n",
    "                filtred_preds = preds[i][preds[i]>4]\n",
    "                filtred_targets = target_input_ids[i][:len(filtred_preds)]\n",
    "                pred_trgs.extend([filtred_preds.tolist()])\n",
    "                targets.extend([filtred_targets.tolist()])\n",
    "    return pred_trgs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac64aeb3-818f-450b-84f5-ca3d1e6a8097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring: 100%|██████████| 8/8 [00:05<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_trgs, targets = get_sequences_lig_doctor(model, test_dataloader, max_len = 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1d5e07e-d309-4857-b0f3-ca05d990f267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sifal.klioui/PatientTrajectoryForecasting\n"
     ]
    }
   ],
   "source": [
    "cd PatientTrajectoryForecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77999498-88bc-48e4-8588-d61603cf371b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_map@20': 0.26531395405940383, 'test_map@40': 0.2282488541575416, 'test_map@60': 0.2109436663301642} {'test_recall@20': 0.15444687756068506, 'test_recall@40': 0.27659506717594756, 'test_recall@60': 0.353855012723892}\n"
     ]
    }
   ],
   "source": [
    "from utils.eval import mapk, recallTop\n",
    "\n",
    "\n",
    "ks = [20, 40, 60]\n",
    "\n",
    "test_mapk = {f\"test_map@{k}\": mapk(targets, pred_trgs, k) for k in ks}\n",
    "test_recallk = {f\"test_recall@{k}\": recallTop(targets, pred_trgs, rank = [k])[0] for k in ks}\n",
    "print(test_mapk, test_recallk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2980ddf-8ae3-40c6-b15e-f8b4e766ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.topk(output, k = max_len).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "46291d79-b53b-475a-bc1c-fc16853d4631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader.batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2627e59c-8fab-4313-8948-59fb93c48c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "37992d78-2d2d-4274-a436-43dd01c6a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test_dataloader.batch_size):\n",
    "    filtred_preds = preds[i][preds[i]>4]\n",
    "    filtred_targets = target_input_ids[i][:len(filtred_preds)]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd0e95a6-ac8f-4e40-b53f-7b7a9849596f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtred_targets) == len(filtred_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7f64e-f049-4b3e-9336-0955a0a32990",
   "metadata": {},
   "outputs": [],
   "source": [
    "off = []\n",
    "off.extend("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9650591e-d243-4205-80a3-c8315147ee51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 96])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7fb0587e-6e06-408f-90ea-ee0df858a5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds[i][preds[i]>4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978053c8-0297-423f-af24-5af41f165cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_input_ids[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebe043-2ecb-4304-9a50-e0a16bd13cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78b665b4-d640-4c19-9be1-9e8707130e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0e5163c-5d85-4fe5-a655-8c378e45dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_stats(targets: List[List[int]], seq_len : int = 96, ks : List[int] = [20, 40, 60], num_runs_avg : int = 5):\n",
    "    \"\"\"\n",
    "    Returns the average MAP@k and Recall@k scores for a random forecasting model.\n",
    "\n",
    "    Args:\n",
    "        targets (List[List[int]]): The list of target sequences.\n",
    "        seq_len (int, optional): The length of the forecasted sequence. Defaults to 96.\n",
    "        ks (List[int], optional): The list of k values for MAP@k and Recall@k. Defaults to [20, 40, 60].\n",
    "        num_runs_avg (int, optional): The number of runs to average the results over. Defaults to 5.\n",
    "    Returns:\n",
    "        Dict[str, float], Dict[str, float]: The average MAP@k and Recall@k scores.\n",
    "    \"\"\"\n",
    "    # targets = [concated_dt[i]['target_sequences'].numpy().tolist() for i in range(len(concated_dt))]\n",
    "    unique_targets = list(set([item for sublist in targets for item in sublist]))\n",
    "    \n",
    "    cumulative_mapk = {f\"test_map@{k}\": 0.0 for k in ks}\n",
    "    cumulative_recallk = {f\"test_recall@{k}\": 0.0 for k in ks}\n",
    "    \n",
    "    for _ in range(num_runs_avg):\n",
    "        \n",
    "        forecasted = [np.random.choice(unique_targets, size=seq_len, replace=False).tolist() for _ in range(len(targets))]\n",
    "    \n",
    "        run_mapk = {f\"test_map@{k}\": mapk(targets, forecasted, k) for k in ks}\n",
    "        run_recallk = {f\"test_recall@{k}\": recallTop(targets, forecasted, rank=[k])[0] for k in ks}\n",
    "    \n",
    "        # Accumulate results\n",
    "        for k in ks:\n",
    "            cumulative_mapk[f\"test_map@{k}\"] += run_mapk[f\"test_map@{k}\"]\n",
    "            cumulative_recallk[f\"test_recall@{k}\"] += run_recallk[f\"test_recall@{k}\"]\n",
    "    \n",
    "    # Compute average results\n",
    "    average_mapk = {f\"test_map@{k}\": cumulative_mapk[f\"test_map@{k}\"] / num_runs_avg for k in ks}\n",
    "    average_recallk = {f\"test_recall@{k}\": cumulative_recallk[f\"test_recall@{k}\"] / num_runs_avg for k in ks}\n",
    "\n",
    "    return average_mapk, average_recallk    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8e2ff25-ef5a-48c0-8a9d-359ab9f3777e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'test_map@20': 0.05347963114250881,\n",
       "  'test_map@40': 0.0573658107319462,\n",
       "  'test_map@60': 0.05544175641942543},\n",
       " {'test_recall@20': 0.029036955706191657,\n",
       "  'test_recall@40': 0.05811648229377542,\n",
       "  'test_recall@60': 0.08716030403904587})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_stats(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "727eb57f-941d-4802-9412-f4cfdfc9c734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 96)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(forecasted[0]), len(set(forecasted[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c127c53-e0b2-4edd-aff5-78fec982c707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crossval",
   "language": "python",
   "name": "crossval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
